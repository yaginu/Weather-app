{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d222b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow-transform\n",
    "!pip install tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7260a2b8",
   "metadata": {
    "id": "utSiowwFiXhA"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48062020",
   "metadata": {
    "id": "9aedf061-4fe2-4948-a93b-f5b8ab1ebedd"
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Loading dataset\n",
    "def load_dataset(filename, batch_size, mode):\n",
    "\n",
    "    # Setting defaults\n",
    "    CSV_COLUMNS = [\n",
    "        'day_sin', 'day_cos', 'year_sin', 'year_cos', 'air_pressure_ashore', 'air_pressure_afloat', 'diff_air_pressure',\n",
    "        'precipitation', 'temperature', 'humidity', 'wind_vector_x', \"wind_vector_y\", \n",
    "        'hours_of_daylight', 'global_solar_radiation', 'temp_mean', 'temp_var'\n",
    "    ]\n",
    "\n",
    "    SELECT_COLUMNS = [\n",
    "        'day_sin', 'day_cos', 'year_sin', 'year_cos', 'air_pressure_ashore', 'air_pressure_afloat', 'diff_air_pressure',\n",
    "        'precipitation', 'temperature', 'humidity', 'wind_vector_x', \"wind_vector_y\", 'hours_of_daylight', 'global_solar_radiation'\n",
    "    ]\n",
    "\n",
    "    DEFAULTS = [[0.0] for _ in range(len(SELECT_COLUMNS))]\n",
    "    \n",
    "    # Packing features\n",
    "    def pack(features):\n",
    "        packed_features =  tf.stack(list(features.values()), axis=1)\n",
    "\n",
    "        return tf.reshape(packed_features, [-1])\n",
    "    \n",
    "    @tf.function\n",
    "    def marshal(x, feature_keys):\n",
    "        features = {\n",
    "            k: x[:, feature_keys.index(k)] for k in feature_keys\n",
    "        }\n",
    "        \n",
    "        return features\n",
    "\n",
    "    # Window processing\n",
    "    def windowed_dataset(dataset, batch_size, mode):\n",
    "        \n",
    "        marshal_fn_partial = partial(marshal, feature_keys=SELECT_COLUMNS) \n",
    "        \n",
    "        dataset = dataset.map(pack)\n",
    "        dataset = dataset.window(size=48, shift=1, drop_remainder=True)\n",
    "        dataset = dataset.flat_map(lambda window: window.batch(48))\n",
    "\n",
    "        if mode == \"train\":\n",
    "            dataset.shuffle(1000)\n",
    "        \n",
    "        encoder_input = dataset.map(lambda window: window[:24]).map(marshal_fn_partial)\n",
    "        decoder_input = dataset.map(lambda window: tf.concat((tf.zeros((1)), window[24:-1, 8]), axis=0))\n",
    "        decoder_output = dataset.map(lambda window: window[24:, 8])\n",
    "\n",
    "        inputs = tf.data.Dataset.zip((encoder_input, decoder_input))\n",
    "        dataset = tf.data.Dataset.zip((inputs, decoder_output)).cache()\n",
    "            \n",
    "        dataset = dataset.batch(batch_size, drop_remainder=True).repeat(1).prefetch(1)  \n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "            file_pattern=filename,\n",
    "            column_names=CSV_COLUMNS,\n",
    "            column_defaults=DEFAULTS,\n",
    "            select_columns=SELECT_COLUMNS,\n",
    "            batch_size=1,\n",
    "            shuffle=False,\n",
    "            num_epochs=1)\n",
    "\n",
    "    dataset = windowed_dataset(dataset, batch_size, mode)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd573c2a",
   "metadata": {
    "id": "62ba21b7-f695-4f34-bee1-c0ec0f5ec7b8"
   },
   "outputs": [],
   "source": [
    "training_file_path = \"############\"\n",
    "validation_file_path = \"############\"\n",
    "\n",
    "transform_artefacts_dir = \"############\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0880c838",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_dataset(training_file_path, 256, \"train\")\n",
    "valid_dataset = load_dataset(validation_file_path, 128, \"eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5e6f95",
   "metadata": {
    "id": "e36f993e-3a4f-412c-88d8-c7d8891eb179"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "# Creating model for training and evaluating\n",
    "def train_model(num_units=128, learning_rate=0.001, dropout_rate=0.35):\n",
    "    \n",
    "    SELECT_COLUMNS = [\n",
    "        'day_sin', 'day_cos', 'year_sin', 'year_cos', 'air_pressure_ashore', 'air_pressure_afloat', 'diff_air_pressure',\n",
    "        'precipitation', 'temperature', 'humidity', 'wind_vector_x', \"wind_vector_y\", 'hours_of_daylight', 'global_solar_radiation'\n",
    "    ]\n",
    "    \n",
    "    # Input layer\n",
    "    encoder_input_layers = {\n",
    "        colname: tf.keras.layers.Input(name=colname, shape=(24, 1), dtype=tf.float32)\n",
    "        for colname in SELECT_COLUMNS\n",
    "    }\n",
    "    \n",
    "    pre_model_input = tf.keras.layers.Concatenate(axis=-1, name=\"concatenate\")(encoder_input_layers.values())\n",
    "    \n",
    "    # Encoder\n",
    "    encoder_lstm = tf.keras.layers.LSTM(num_units, return_sequences=True, name=\"encoder_lstm1\")(pre_model_input)\n",
    "    encoder_dropout = tf.keras.layers.Dropout(dropout_rate, name=\"encoder_dropout\")(encoder_lstm)\n",
    "    encoder_output, state_h, state_c = tf.keras.layers.LSTM(num_units, return_state=True, name=\"encoder_lstm2\")(encoder_dropout)\n",
    "    encoder_state = [state_h, state_c]\n",
    "\n",
    "    # Sampler\n",
    "    sampler = tfa.seq2seq.sampler.ScheduledOutputTrainingSampler(\n",
    "        sampling_probability=0.,\n",
    "        next_inputs_fn=lambda outputs: tf.reshape(outputs, shape=(1, 1))\n",
    "    )\n",
    "    sampler.sampling_probability = tf.Variable(0.)\n",
    "\n",
    "    # Decoder\n",
    "    decoder_input = tf.keras.layers.Input(shape=(24, 1), name=\"decoder_input\")\n",
    "\n",
    "    decoder_cell = tf.keras.layers.LSTMCell(num_units, name=\"decoder_lstm\")\n",
    "    output_layer = tf.keras.layers.Dense(1, name=\"decoder_output\")\n",
    "\n",
    "    decoder = tfa.seq2seq.basic_decoder.BasicDecoder(decoder_cell, sampler, output_layer=output_layer)\n",
    "    decoder_output, _, _ = decoder(decoder_input, initial_state=encoder_state, sequence_length=[24])\n",
    "\n",
    "    final_output = decoder_output.rnn_output\n",
    "\n",
    "    # Creating model\n",
    "    model = tf.keras.Model(\n",
    "        inputs=[encoder_input_layers, decoder_input], outputs=[final_output])\n",
    "\n",
    "    optimizer = tf.keras.optimizers.RMSprop(learning_rate)\n",
    "    model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "\n",
    "    return model, encoder_input_layers, encoder_state, decoder_cell, output_layer, sampler\n",
    "\n",
    "\n",
    "# Creating model for prediction\n",
    "def predict_model(encoder_input_layers, encoder_state, decoder_cell, output_layer):\n",
    "    \n",
    "    # Encoder Layer Class\n",
    "    class Inference_Encoder(tf.keras.layers.Layer):\n",
    "        def __init__(self, encoder_input_layers, encoder_state):\n",
    "            super().__init__()\n",
    "\n",
    "            self.model = tf.keras.models.Model(inputs=[encoder_input_layers], outputs=encoder_state)\n",
    "\n",
    "        @tf.function\n",
    "        def call(self, inputs):\n",
    "\n",
    "            return self.model(inputs)\n",
    "\n",
    "    # Decoder Layer Class\n",
    "    class Inference_Decoder(tf.keras.layers.Layer):\n",
    "\n",
    "        def __init__(self, decoder_cell, output_layer):\n",
    "            super().__init__()\n",
    "\n",
    "            # Inference sampler\n",
    "            self.sampler = tfa.seq2seq.sampler.InferenceSampler(\n",
    "                sample_fn = lambda outputs: tf.reshape(outputs, (1, 1)),\n",
    "                sample_shape = [1],\n",
    "                sample_dtype = tf.float32,\n",
    "                end_fn = lambda sample_ids : False,\n",
    "            )\n",
    "\n",
    "            self.decoder = tfa.seq2seq.basic_decoder.BasicDecoder(\n",
    "                decoder_cell, self.sampler, output_layer=output_layer, maximum_iterations=24\n",
    "            )\n",
    "\n",
    "        @tf.function\n",
    "        def call(self, initial_state):\n",
    "            start_inputs = tf.zeros(shape=(1, 1))\n",
    "            decoder_output, _, _ = self.decoder(start_inputs, initial_state=initial_state)\n",
    "            final_output = decoder_output.rnn_output\n",
    "\n",
    "            return final_output\n",
    "\n",
    "    # Inference Model Class\n",
    "    class Inference_Model(tf.keras.Model):\n",
    "        def __init__(self, encoder_input_layers, encoder_state, decoder_cell, output_layer):\n",
    "            super().__init__()\n",
    "\n",
    "            self.encoder = Inference_Encoder(encoder_input_layers, encoder_state)\n",
    "            self.decoder = Inference_Decoder(decoder_cell, output_layer)\n",
    "\n",
    "        @tf.function\n",
    "        def call(self, inputs):\n",
    "            \n",
    "            inputs_copy = inputs.copy()\n",
    "            \n",
    "            temp_mean = inputs_copy.pop('temp_mean')[0][0]\n",
    "            temp_var = inputs_copy.pop('temp_var')[0][0]\n",
    "\n",
    "            initial_state = self.encoder(inputs_copy)\n",
    "            outputs = self.decoder(initial_state)\n",
    "            \n",
    "            outputs_rescaled = outputs * tf.sqrt(temp_var) + temp_mean\n",
    "\n",
    "            return outputs_rescaled\n",
    "        \n",
    "    inference_model = Inference_Model(encoder_input_layers, encoder_state, decoder_cell, output_layer)\n",
    "\n",
    "    return inference_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392d80b1",
   "metadata": {
    "id": "vrsXcTMjBBWK"
   },
   "outputs": [],
   "source": [
    "model, encoder_input_layers, encoder_state, decoder_cell, output_layer, sampler = train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd83418",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_sampling_probability(epoch, logs):\n",
    "    eps = 1e-16\n",
    "    proba = max(0.0, min(1.0, epoch / (num_epochs - 10 + eps)))\n",
    "    sampler.sampling_probability.assign(proba)\n",
    "\n",
    "sampling_probability_cb = tf.keras.callbacks.LambdaCallback(on_epoch_begin=update_sampling_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57700019",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "\n",
    "history = model.fit(train_dataset, epochs=num_epochs,\n",
    "                    validation_data=valid_dataset,\n",
    "                    callbacks=sampling_probability_cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5b2b96",
   "metadata": {
    "id": "6b8253a0-85da-4586-b540-d6b6464b48b5"
   },
   "outputs": [],
   "source": [
    "inference_model = predict_model(encoder_input_layers, encoder_state, decoder_cell, output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a45cd7",
   "metadata": {
    "id": "_6addWldVlRx"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def export_serving_model(model, tf_transform_output, out_dir):\n",
    "\n",
    "    TRANSFORM_FEATURE_COLUMNS = [\n",
    "        'Date', 'air_pressure_ashore', 'air_pressure_afloat', 'precipitation', 'temperature',\n",
    "        'humidity', 'wind_direction', 'wind_velocity', 'hours_of_daylight', 'global_solar_radiation'\n",
    "    ]\n",
    "\n",
    "    SELECT_COLUMNS = [\n",
    "        'day_sin', 'day_cos', 'year_sin', 'year_cos', 'air_pressure_ashore', 'air_pressure_afloat', 'diff_air_pressure',\n",
    "        'precipitation', 'temperature', 'humidity', 'wind_vector_x', \"wind_vector_y\", 'hours_of_daylight', 'global_solar_radiation',\n",
    "        'temp_mean', 'temp_var'\n",
    "    ]\n",
    "    \n",
    "    # Building Model\n",
    "    example = {\n",
    "        x: tf.random.uniform(shape=(1, 24), name=x)\n",
    "        for x in SELECT_COLUMNS\n",
    "    }\n",
    "    ex = model(example)\n",
    "    \n",
    "    # Transform raw features\n",
    "    def get_apply_tft_layer(tf_transform_output):\n",
    "        \n",
    "        tft_layer = tf_transform_output.transform_features_layer()\n",
    "\n",
    "        @tf.function\n",
    "        def apply_tf_transform(raw_features_dict):\n",
    "\n",
    "            unbatched_raw_features = {\n",
    "                k: tf.squeeze(tf.reshape(v, (1, -1)))\n",
    "                for k, v in raw_features_dict.items()\n",
    "            }\n",
    "\n",
    "            transformed_dataset = tft_layer(unbatched_raw_features)\n",
    "\n",
    "            expanded_dims = {\n",
    "                k: tf.reshape(v, (-1, 24))\n",
    "                for k, v in transformed_dataset.items()\n",
    "            }\n",
    "            \n",
    "            return expanded_dims\n",
    "\n",
    "        return apply_tf_transform\n",
    "\n",
    "    def get_serve_raw_fn(model, tf_transform_output):\n",
    "\n",
    "        model.preprocessing_layer = get_apply_tft_layer(tf_transform_output)\n",
    "\n",
    "        @tf.function\n",
    "        def serve_raw_fn(features):\n",
    "\n",
    "            preprocessed_features = model.preprocessing_layer(features)\n",
    "        \n",
    "            return preprocessed_features\n",
    "\n",
    "        return serve_raw_fn\n",
    "    \n",
    "    serving_raw_entry = get_serve_raw_fn(model, tf_transform_output)   \n",
    "    \n",
    "    serving_transform_signature_tensorspecs = {\n",
    "        x: tf.TensorSpec(shape=[None, 24], dtype=tf.float32, name=x)\n",
    "        for x in TRANSFORM_FEATURE_COLUMNS\n",
    "    }\n",
    "\n",
    "    serving_signature_tensorspecs = {\n",
    "        x: tf.TensorSpec(shape=[None, 24], dtype=tf.float32, name=x)\n",
    "        for x in SELECT_COLUMNS\n",
    "    }\n",
    "    \n",
    "    # Signatures\n",
    "    signatures = {\n",
    "        'serving_default': model.call.get_concrete_function(serving_signature_tensorspecs),\n",
    "        'transform': serving_raw_entry.get_concrete_function(serving_transform_signature_tensorspecs),\n",
    "                  }\n",
    "\n",
    "    tf.keras.models.save_model(model=model, filepath=out_dir, signatures=signatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442ef852",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_transform_output = tft.TFTransformOutput(transform_artefacts_dir)\n",
    "tft_layer = tf_transform_output.transform_features_layer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0164c2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3C4h237hV9av",
    "outputId": "bd1d20d7-a8a4-4533-a0b2-aea4f948f80d"
   },
   "outputs": [],
   "source": [
    "export_serving_model(inference_model, tf_transform_output, \"export\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d904d6c4",
   "metadata": {
    "id": "rd3EoCwR05cp"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "# Loading dataset\n",
    "def load_test_dataset(filename, batch_size):\n",
    "\n",
    "    CSV_COLUMNS = [\n",
    "        'Date', 'air_pressure_ashore', 'air_pressure_afloat', 'precipitation', 'temperature',\n",
    "        'humidity', 'wind_direction', 'wind_velocity', 'hours_of_daylight', 'global_solar_radiation',\n",
    "        'weather', 'cloud cover'\n",
    "    ]\n",
    "\n",
    "    SELECT_COLUMNS = [\n",
    "        'Date', 'air_pressure_ashore', 'air_pressure_afloat', 'precipitation', 'temperature',\n",
    "        'humidity', 'wind_direction', 'wind_velocity', 'hours_of_daylight', 'global_solar_radiation',\n",
    "    ]\n",
    "\n",
    "    DEFAULTS = [[0.0] for i in SELECT_COLUMNS]\n",
    "\n",
    "    # Packing features\n",
    "    def pack(features):\n",
    "        packed_features =  tf.stack(list(features.values()), axis=1)\n",
    "\n",
    "        return tf.reshape(packed_features, [-1])\n",
    "    \n",
    "    @tf.function\n",
    "    def marshal(x, feature_keys):\n",
    "        features = {\n",
    "            k: x[:, feature_keys.index(k)] for k in feature_keys\n",
    "        }\n",
    "        \n",
    "        return features\n",
    "\n",
    "    # Window processing\n",
    "    def windowed_dataset(dataset, batch_size):\n",
    "        \n",
    "        marshal_fn_partial = partial(marshal, feature_keys=SELECT_COLUMNS) \n",
    "\n",
    "        dataset = dataset.map(pack)\n",
    "        dataset = dataset.window(size=48, shift=1, drop_remainder=True)\n",
    "        dataset = dataset.flat_map(lambda window: window.batch(48))\n",
    "            \n",
    "        x_test = dataset.map(lambda window: window[:24]).map(marshal_fn_partial).batch(batch_size, drop_remainder=True).repeat(1).prefetch(1)  \n",
    "        y_true = dataset.map(lambda window: window[24:, 4]).batch(batch_size, drop_remainder=True).repeat(1).prefetch(1)  \n",
    "        \n",
    "        dataset = tf.data.Dataset.zip((x_test, y_true)).cache()\n",
    "        dataset = dataset.batch(batch_size, drop_remainder=True).repeat(1).prefetch(1)  \n",
    "        \n",
    "        # return dataset   \n",
    "        return x_test, y_true\n",
    "    \n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "            file_pattern=filename,\n",
    "            column_names=CSV_COLUMNS,\n",
    "            column_defaults=DEFAULTS,\n",
    "            select_columns=SELECT_COLUMNS,\n",
    "            batch_size=1,\n",
    "            shuffle=False,\n",
    "            num_epochs=1,\n",
    "            header=False\n",
    "    )\n",
    "\n",
    "    x_test, y_true = windowed_dataset(dataset, batch_size)\n",
    "\n",
    "    return x_test, y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46d03ff",
   "metadata": {
    "id": "qhhMNtF025Gi"
   },
   "outputs": [],
   "source": [
    "x_test, y_true = load_test_dataset(\"################\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72929706",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vQ2jMjwBWgBB",
    "outputId": "0440d367-6adb-49e7-8a67-9373ccded41b"
   },
   "outputs": [],
   "source": [
    "loaded_model = tf.keras.models.load_model(\"export\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fc06c5",
   "metadata": {
    "id": "XPZSue2Sf_oU"
   },
   "outputs": [],
   "source": [
    "x_test_transformed = x_test.map(loaded_model.preprocessing_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f702c2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.predict(next(iter(x_test_transformed)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc78a41",
   "metadata": {
    "id": "5fDFfcttXzmH"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "prediction = []\n",
    "for item in x_test_transformed:\n",
    "    prediction.append(loaded_model.predict(item))\n",
    "\n",
    "y_pred = np.array(prediction).reshape(-1, 24)\n",
    "y_pred_rescaled = y_pred * tf.sqrt(temp_var) + temp_mean\n",
    "\n",
    "y_true = np.array(list(tf.data.Dataset.as_numpy_iterator(y_true))).reshape(-1, 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b90a8f8",
   "metadata": {
    "id": "PiqIGR1c0nNy"
   },
   "outputs": [],
   "source": [
    "def calculate_loss(y_pred, y_true):\n",
    "    \n",
    "    mse = tf.keras.losses.MeanSquaredError()\n",
    "    \n",
    "    return mse(y_true, y_pred).numpy().astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b27dd53",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ay8iNZNllnWC",
    "outputId": "f04e3f7f-3e56-4446-bd09-5f381810df1e"
   },
   "outputs": [],
   "source": [
    "metric_value = calculate_loss(y_pred_rescaled, y_true)\n",
    "print(metric_value)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "model.ipynb",
   "provenance": []
  },
  "environment": {
   "name": "tf2-gpu.2-5.m75",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-5:m75"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
