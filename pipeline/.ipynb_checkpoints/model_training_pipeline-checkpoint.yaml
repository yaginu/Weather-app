apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: weather-forecast-model-training-
  annotations: {pipelines.kubeflow.org/kfp_sdk_version: 1.6.5, pipelines.kubeflow.org/pipeline_compilation_time: '2021-07-29T06:23:35.225536',
    pipelines.kubeflow.org/pipeline_spec: '{"description": "The pipeline training
      and deploying the Weather-forecast pipeline", "inputs": [{"name": "project_id"},
      {"name": "gcs_root"}, {"name": "region"}, {"name": "source_table_name"}, {"name":
      "num_epochs_hypertune"}, {"name": "num_epochs_retrain"}, {"name": "num_units"},
      {"name": "evaluation_metric_name"}, {"name": "evaluation_metric_threshold"},
      {"name": "model_id"}, {"name": "version_id"}, {"name": "replace_existing_version"},
      {"default": "\n{\n    \"hyperparameters\": {\n        \"goal\": \"MINIMIZE\",\n        \"maxTrials\":
      3,\n        \"maxParallelTrials\": 3,\n        \"hyperparameterMetricTag\":
      \"val_loss\",\n        \"enableTrialEarlyStopping\": True,\n        \"params\":[\n            {\n                \"parameterName\":
      \"learning_rate\",\n                \"type\": \"DOUBLE\",\n                \"minValue\":
      0.00001,\n                \"maxValue\": 0.1,\n                \"scaleType\":
      \"UNIT_LOG_SCALE\"\n            },\n            {\n                \"parameterName\":
      \"dropout_rate\",\n                \"type\": \"DOUBLE\",\n                \"minValue\":
      0.1,\n                \"maxValue\": 0.4,\n                \"scaleType\": \"UNIT_LOG_SCALE\"\n            }\n        ]\n    }\n}\n",
      "name": "hypertune_settings", "optional": true}], "name": "Weather-forecast
      Model Training"}'}
  labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.6.5}
spec:
  entrypoint: weather-forecast-model-training
  templates:
  - name: condition-1
    inputs:
      parameters:
      - {name: model_id}
      - {name: project_id}
      - {name: region}
      - {name: replace_existing_version}
      - {name: submitting-a-cloud-ml-training-job-as-a-pipeline-step-2-job_dir}
      - {name: version_id}
    dag:
      tasks:
      - name: deploying-a-trained-model-to-cloud-machine-learning-engine
        template: deploying-a-trained-model-to-cloud-machine-learning-engine
        arguments:
          parameters:
          - {name: model_id, value: '{{inputs.parameters.model_id}}'}
          - {name: project_id, value: '{{inputs.parameters.project_id}}'}
          - {name: region, value: '{{inputs.parameters.region}}'}
          - {name: replace_existing_version, value: '{{inputs.parameters.replace_existing_version}}'}
          - {name: submitting-a-cloud-ml-training-job-as-a-pipeline-step-2-job_dir,
            value: '{{inputs.parameters.submitting-a-cloud-ml-training-job-as-a-pipeline-step-2-job_dir}}'}
          - {name: version_id, value: '{{inputs.parameters.version_id}}'}
  - name: deploying-a-trained-model-to-cloud-machine-learning-engine
    container:
      args:
      - --ui_metadata_path
      - /tmp/outputs/MLPipeline_UI_metadata/data
      - kfp_component.google.ml_engine
      - deploy
      - --model_uri
      - '{{inputs.parameters.submitting-a-cloud-ml-training-job-as-a-pipeline-step-2-job_dir}}/predict'
      - --project_id
      - '{{inputs.parameters.project_id}}'
      - --model_id
      - '{{inputs.parameters.model_id}}'
      - --version_id
      - '{{inputs.parameters.version_id}}'
      - --runtime_version
      - '2.5'
      - --python_version
      - '3.7'
      - --model
      - '{"onlinePredictionConsoleLogging": true, "regions": ["{{inputs.parameters.region}}"]}'
      - --version
      - '{"packageUris": ["gs://##########/staging/dist/my_custom_code-0.1.tar.gz"],
        "predictionClass": "predictor.MyPredictor"}'
      - --replace_existing_version
      - '{{inputs.parameters.replace_existing_version}}'
      - --set_default
      - "False"
      - --wait_interval
      - '30'
      - --model_uri_output_path
      - /tmp/outputs/model_uri/data
      - --model_name_output_path
      - /tmp/outputs/model_name/data
      - --version_name_output_path
      - /tmp/outputs/version_name/data
      command: []
      env:
      - {name: KFP_POD_NAME, value: '{{pod.name}}'}
      - name: KFP_POD_NAME
        valueFrom:
          fieldRef: {fieldPath: metadata.name}
      - name: KFP_NAMESPACE
        valueFrom:
          fieldRef: {fieldPath: metadata.namespace}
      - name: WORKFLOW_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''workflows.argoproj.io/workflow'']'}
      - name: ENABLE_CACHING
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipelines.kubeflow.org/enable_caching'']'}
      image: gcr.io/ml-pipeline/ml-pipeline-gcp:1.6.0
    inputs:
      parameters:
      - {name: model_id}
      - {name: project_id}
      - {name: region}
      - {name: replace_existing_version}
      - {name: submitting-a-cloud-ml-training-job-as-a-pipeline-step-2-job_dir}
      - {name: version_id}
    outputs:
      artifacts:
      - {name: mlpipeline-ui-metadata, path: /tmp/outputs/MLPipeline_UI_metadata/data}
      - {name: deploying-a-trained-model-to-cloud-machine-learning-engine-model_name,
        path: /tmp/outputs/model_name/data}
      - {name: deploying-a-trained-model-to-cloud-machine-learning-engine-model_uri,
        path: /tmp/outputs/model_uri/data}
      - {name: deploying-a-trained-model-to-cloud-machine-learning-engine-version_name,
        path: /tmp/outputs/version_name/data}
    metadata:
      labels:
        add-pod-env: "true"
        pipelines.kubeflow.org/kfp_sdk_version: 1.6.5
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "A Kubeflow
          Pipeline component to deploy a trained model from a Cloud Storage\npath
          to a Cloud Machine Learning Engine service.\n", "implementation": {"container":
          {"args": ["--ui_metadata_path", {"outputPath": "MLPipeline UI metadata"},
          "kfp_component.google.ml_engine", "deploy", "--model_uri", {"inputValue":
          "model_uri"}, "--project_id", {"inputValue": "project_id"}, "--model_id",
          {"inputValue": "model_id"}, "--version_id", {"inputValue": "version_id"},
          "--runtime_version", {"inputValue": "runtime_version"}, "--python_version",
          {"inputValue": "python_version"}, "--model", {"inputValue": "model"}, "--version",
          {"inputValue": "version"}, "--replace_existing_version", {"inputValue":
          "replace_existing_version"}, "--set_default", {"inputValue": "set_default"},
          "--wait_interval", {"inputValue": "wait_interval"}, "--model_uri_output_path",
          {"outputPath": "model_uri"}, "--model_name_output_path", {"outputPath":
          "model_name"}, "--version_name_output_path", {"outputPath": "version_name"}],
          "env": {"KFP_POD_NAME": "{{pod.name}}"}, "image": "gcr.io/ml-pipeline/ml-pipeline-gcp:1.6.0"}},
          "inputs": [{"description": "Required. The Cloud Storage URI which contains
          a model file. Commonly  used TF model search paths (export/exporter) will
          be used if they exist.", "name": "model_uri", "type": "GCSPath"}, {"description":
          "Required.The ID of the parent project of the serving model.", "name": "project_id",
          "type": "GCPProjectID"}, {"default": "", "description": "Optional. The user-specified
          name of the model. If it is not provided,  the operation uses a random name.",
          "name": "model_id", "type": "String"}, {"default": "", "description": "Optional.
          The user-specified name of the version. If it is not provided,  the operation
          uses a random name.", "name": "version_id", "type": "String"}, {"default":
          "", "description": "Optional. The [Cloud ML Engine runtime version](https://cloud.google.com/ml-engine/docs/tensorflow/runtime-version-list)
          to use for  this deployment. If it is not set, the Cloud ML Engine uses
          the default  stable version, 1.0.", "name": "runtime_version", "type": "String"},
          {"default": "", "description": "Optional. The version of Python used in
          the prediction. If it is not set,  the default version is `2.7`. Python
          `3.5` is available when the  runtime_version is set to `1.4` and above.
          Python `2.7` works with all  supported runtime versions.", "name": "python_version",
          "type": "String"}, {"default": "", "description": "Optional. The JSON payload
          of the new  [Model](https://cloud.google.com/ml-engine/reference/rest/v1/projects.models),
          if it does not exist.", "name": "model", "type": "Dict"}, {"default": "",
          "description": "Optional. The JSON payload of the new  [Version](https://cloud.google.com/ml-engine/reference/rest/v1/projects.models.versions).",
          "name": "version", "type": "Dict"}, {"default": "False", "description":
          "A Boolean flag that indicates whether to replace existing version in case
          of conflict.", "name": "replace_existing_version", "type": "Bool"}, {"default":
          "False", "description": "A Boolean flag that indicates whether to set the
          new version as default version in the model.", "name": "set_default", "type":
          "Bool"}, {"default": "30", "description": "A time-interval to wait for in
          case the operation has a long run time.", "name": "wait_interval", "type":
          "Integer"}], "metadata": {"labels": {"add-pod-env": "true"}}, "name": "Deploying
          a trained model to Cloud Machine Learning Engine", "outputs": [{"description":
          "The Cloud Storage URI of the trained model.", "name": "model_uri", "type":
          "GCSPath"}, {"description": "The name of the deployed model.", "name": "model_name",
          "type": "String"}, {"description": "The name of the deployed version.",
          "name": "version_name", "type": "String"}, {"name": "MLPipeline UI metadata",
          "type": "UI metadata"}]}', pipelines.kubeflow.org/component_ref: '{"digest":
          "a1ceb37c0b8d18f218debabe743c6c14c1acb8e4dd284c9238f0f193c87a33ec", "name":
          "ml_engine/deploy", "url": "https://raw.githubusercontent.com/kubeflow/pipelines/1.6.0/components/gcp/ml_engine/deploy/component.yaml"}',
        pipelines.kubeflow.org/arguments.parameters: '{"model": "{\"onlinePredictionConsoleLogging\":
          true, \"regions\": [\"{{inputs.parameters.region}}\"]}", "model_id": "{{inputs.parameters.model_id}}",
          "model_uri": "{{inputs.parameters.submitting-a-cloud-ml-training-job-as-a-pipeline-step-2-job_dir}}/predict",
          "project_id": "{{inputs.parameters.project_id}}", "python_version": "3.7",
          "replace_existing_version": "{{inputs.parameters.replace_existing_version}}",
          "runtime_version": "2.5", "set_default": "False", "version": "{\"packageUris\":
          [\"gs://###########/staging/dist/my_custom_code-0.1.tar.gz\"],
          \"predictionClass\": \"predictor.MyPredictor\"}", "version_id": "{{inputs.parameters.version_id}}",
          "wait_interval": "30"}'}
  - name: evaluate-model
    container:
      args: [--dataset-path, '{{inputs.parameters.run-transformation-pipeline-testing_file_path}}',
        --model-path, '{{inputs.parameters.submitting-a-cloud-ml-training-job-as-a-pipeline-step-2-job_dir}}',
        --transform-artefacts-dir, '{{inputs.parameters.run-transformation-pipeline-transform_artefacts_dir}}',
        --metric-name, '{{inputs.parameters.evaluation_metric_name}}', '----output-paths',
        /tmp/outputs/metric_name/data, /tmp/outputs/metric_value/data, /tmp/outputs/mlpipeline_metrics/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def evaluate_model(\n    dataset_path, model_path, transform_artefacts_dir,\
        \ metric_name\n):\n\n    import json\n\n    import tensorflow as tf\n    import\
        \ numpy as np\n\n    from create_dataset import load_test_dataset\n\n    def\
        \ calculate_loss(y_pred, y_true):\n\n        mse = tf.keras.losses.MeanSquaredError()\n\
        \n        return mse(y_true, y_pred).numpy().astype(np.float64)\n\n    model_path\
        \ = '{}/predict'.format(model_path)\n    model = tf.keras.models.load_model(model_path)\n\
        \n    x_test, y_true = load_test_dataset(dataset_path + \"*\", 256)\n\n  \
        \  x_test_transformed = x_test.map(model.preprocessing_layer)\n\n    prediction\
        \ = []\n    for item in x_test_transformed:\n        prediction.append(model.predict(item))\n\
        \n    y_pred = np.array(prediction).reshape(-1, 24)        \n    y_true =\
        \ np.array(list(tf.data.Dataset.as_numpy_iterator(y_true))).reshape(-1, 24)\n\
        \n    if metric_name == \"mse\":\n        metric_value = calculate_loss(y_pred,\
        \ y_true)\n        print(\"metric_value:\", metric_value)\n\n    else:\n \
        \       metric_name = 'N/A'\n        metric_value = 0\n\n    metrics = {\n\
        \        'metrics': [{\n            'name': metric_name,\n            'numberValue':\
        \ metric_value\n        }]\n    }\n\n    return (metric_name, metric_value,\
        \ json.dumps(metrics))\n\ndef _serialize_float(float_value: float) -> str:\n\
        \    if isinstance(float_value, str):\n        return float_value\n    if\
        \ not isinstance(float_value, (float, int)):\n        raise TypeError('Value\
        \ \"{}\" has type \"{}\" instead of float.'.format(str(float_value), str(type(float_value))))\n\
        \    return str(float_value)\n\ndef _serialize_str(str_value: str) -> str:\n\
        \    if not isinstance(str_value, str):\n        raise TypeError('Value \"\
        {}\" has type \"{}\" instead of str.'.format(str(str_value), str(type(str_value))))\n\
        \    return str_value\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Evaluate\
        \ model', description='')\n_parser.add_argument(\"--dataset-path\", dest=\"\
        dataset_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --model-path\", dest=\"model_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--transform-artefacts-dir\", dest=\"transform_artefacts_dir\"\
        , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --metric-name\", dest=\"metric_name\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"----output-paths\", dest=\"_output_paths\", type=str,\
        \ nargs=3)\n_parsed_args = vars(_parser.parse_args())\n_output_files = _parsed_args.pop(\"\
        _output_paths\", [])\n\n_outputs = evaluate_model(**_parsed_args)\n\n_output_serializers\
        \ = [\n    _serialize_str,\n    _serialize_float,\n    str,\n\n]\n\nimport\
        \ os\nfor idx, output_file in enumerate(_output_files):\n    try:\n      \
        \  os.makedirs(os.path.dirname(output_file))\n    except OSError:\n      \
        \  pass\n    with open(output_file, 'w') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
      image: gcr.io/##########/docker_images/evaluate_image:latest
    inputs:
      parameters:
      - {name: evaluation_metric_name}
      - {name: run-transformation-pipeline-testing_file_path}
      - {name: run-transformation-pipeline-transform_artefacts_dir}
      - {name: submitting-a-cloud-ml-training-job-as-a-pipeline-step-2-job_dir}
    outputs:
      parameters:
      - name: evaluate-model-metric_value
        valueFrom: {path: /tmp/outputs/metric_value/data}
      artifacts:
      - {name: mlpipeline-metrics, path: /tmp/outputs/mlpipeline_metrics/data}
      - {name: evaluate-model-metric_name, path: /tmp/outputs/metric_name/data}
      - {name: evaluate-model-metric_value, path: /tmp/outputs/metric_value/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.6.5
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--dataset-path", {"inputValue": "dataset_path"}, "--model-path",
          {"inputValue": "model_path"}, "--transform-artefacts-dir", {"inputValue":
          "transform_artefacts_dir"}, "--metric-name", {"inputValue": "metric_name"},
          "----output-paths", {"outputPath": "metric_name"}, {"outputPath": "metric_value"},
          {"outputPath": "mlpipeline_metrics"}], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def evaluate_model(\n    dataset_path, model_path, transform_artefacts_dir,
          metric_name\n):\n\n    import json\n\n    import tensorflow as tf\n    import
          numpy as np\n\n    from create_dataset import load_test_dataset\n\n    def
          calculate_loss(y_pred, y_true):\n\n        mse = tf.keras.losses.MeanSquaredError()\n\n        return
          mse(y_true, y_pred).numpy().astype(np.float64)\n\n    model_path = ''{}/predict''.format(model_path)\n    model
          = tf.keras.models.load_model(model_path)\n\n    x_test, y_true = load_test_dataset(dataset_path
          + \"*\", 256)\n\n    x_test_transformed = x_test.map(model.preprocessing_layer)\n\n    prediction
          = []\n    for item in x_test_transformed:\n        prediction.append(model.predict(item))\n\n    y_pred
          = np.array(prediction).reshape(-1, 24)        \n    y_true = np.array(list(tf.data.Dataset.as_numpy_iterator(y_true))).reshape(-1,
          24)\n\n    if metric_name == \"mse\":\n        metric_value = calculate_loss(y_pred,
          y_true)\n        print(\"metric_value:\", metric_value)\n\n    else:\n        metric_name
          = ''N/A''\n        metric_value = 0\n\n    metrics = {\n        ''metrics'':
          [{\n            ''name'': metric_name,\n            ''numberValue'': metric_value\n        }]\n    }\n\n    return
          (metric_name, metric_value, json.dumps(metrics))\n\ndef _serialize_float(float_value:
          float) -> str:\n    if isinstance(float_value, str):\n        return float_value\n    if
          not isinstance(float_value, (float, int)):\n        raise TypeError(''Value
          \"{}\" has type \"{}\" instead of float.''.format(str(float_value), str(type(float_value))))\n    return
          str(float_value)\n\ndef _serialize_str(str_value: str) -> str:\n    if not
          isinstance(str_value, str):\n        raise TypeError(''Value \"{}\" has
          type \"{}\" instead of str.''.format(str(str_value), str(type(str_value))))\n    return
          str_value\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Evaluate
          model'', description='''')\n_parser.add_argument(\"--dataset-path\", dest=\"dataset_path\",
          type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--model-path\",
          dest=\"model_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--transform-artefacts-dir\",
          dest=\"transform_artefacts_dir\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--metric-name\",
          dest=\"metric_name\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=3)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = evaluate_model(**_parsed_args)\n\n_output_serializers
          = [\n    _serialize_str,\n    _serialize_float,\n    str,\n\n]\n\nimport
          os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "gcr.io/##########/docker_images/evaluate_image:latest"}},
          "inputs": [{"name": "dataset_path", "type": "String"}, {"name": "model_path",
          "type": "String"}, {"name": "transform_artefacts_dir", "type": "String"},
          {"name": "metric_name", "type": "String"}], "name": "Evaluate model", "outputs":
          [{"name": "metric_name", "type": "String"}, {"name": "metric_value", "type":
          "Float"}, {"name": "mlpipeline_metrics", "type": "Metrics"}]}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"dataset_path": "{{inputs.parameters.run-transformation-pipeline-testing_file_path}}",
          "metric_name": "{{inputs.parameters.evaluation_metric_name}}", "model_path":
          "{{inputs.parameters.submitting-a-cloud-ml-training-job-as-a-pipeline-step-2-job_dir}}",
          "transform_artefacts_dir": "{{inputs.parameters.run-transformation-pipeline-transform_artefacts_dir}}"}'}
  - name: retrieve-best-run
    container:
      args: [--project-id, '{{inputs.parameters.project_id}}', --job-id, '{{inputs.parameters.submitting-a-cloud-ml-training-job-as-a-pipeline-step-job_id}}',
        '----output-paths', /tmp/outputs/metric_value/data, /tmp/outputs/learning_rate/data,
        /tmp/outputs/dropout_rate/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def retrieve_best_run(\n    project_id, job_id\n):\n\n    from googleapiclient\
        \ import discovery\n    from googleapiclient import errors\n\n    ml = discovery.build('ml',\
        \ 'v1')\n\n    job_name = 'projects/{}/jobs/{}'.format(project_id, job_id)\n\
        \    request = ml.projects().jobs().get(name=job_name)\n\n    try:\n     \
        \   response = request.execute()\n        print(response)   \n    except errors.HttpError\
        \ as err:\n        print(err)\n    except:\n        print('Unexpected error')\
        \    \n\n    best_trial = response['trainingOutput']['trials'][0]\n\n    print(\"\
        best_trial:\", best_trial)\n\n    metric_value = best_trial['finalMetric']['objectiveValue']\n\
        \    learning_rate = float(best_trial['hyperparameters']['learning_rate'])\n\
        \    dropout_rate = float(best_trial['hyperparameters']['dropout_rate'])\n\
        \n    return (metric_value, learning_rate, dropout_rate)\n\ndef _serialize_float(float_value:\
        \ float) -> str:\n    if isinstance(float_value, str):\n        return float_value\n\
        \    if not isinstance(float_value, (float, int)):\n        raise TypeError('Value\
        \ \"{}\" has type \"{}\" instead of float.'.format(str(float_value), str(type(float_value))))\n\
        \    return str(float_value)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Retrieve\
        \ best run', description='')\n_parser.add_argument(\"--project-id\", dest=\"\
        project_id\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --job-id\", dest=\"job_id\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"----output-paths\", dest=\"_output_paths\", type=str,\
        \ nargs=3)\n_parsed_args = vars(_parser.parse_args())\n_output_files = _parsed_args.pop(\"\
        _output_paths\", [])\n\n_outputs = retrieve_best_run(**_parsed_args)\n\n_output_serializers\
        \ = [\n    _serialize_float,\n    _serialize_float,\n    _serialize_float,\n\
        \n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n    try:\n\
        \        os.makedirs(os.path.dirname(output_file))\n    except OSError:\n\
        \        pass\n    with open(output_file, 'w') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
      image: gcr.io/##########/docker_images/base_image:latest
    inputs:
      parameters:
      - {name: project_id}
      - {name: submitting-a-cloud-ml-training-job-as-a-pipeline-step-job_id}
    outputs:
      parameters:
      - name: retrieve-best-run-dropout_rate
        valueFrom: {path: /tmp/outputs/dropout_rate/data}
      - name: retrieve-best-run-learning_rate
        valueFrom: {path: /tmp/outputs/learning_rate/data}
      artifacts:
      - {name: retrieve-best-run-dropout_rate, path: /tmp/outputs/dropout_rate/data}
      - {name: retrieve-best-run-learning_rate, path: /tmp/outputs/learning_rate/data}
      - {name: retrieve-best-run-metric_value, path: /tmp/outputs/metric_value/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.6.5
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--project-id", {"inputValue": "project_id"}, "--job-id", {"inputValue":
          "job_id"}, "----output-paths", {"outputPath": "metric_value"}, {"outputPath":
          "learning_rate"}, {"outputPath": "dropout_rate"}], "command": ["sh", "-ec",
          "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def retrieve_best_run(\n    project_id,
          job_id\n):\n\n    from googleapiclient import discovery\n    from googleapiclient
          import errors\n\n    ml = discovery.build(''ml'', ''v1'')\n\n    job_name
          = ''projects/{}/jobs/{}''.format(project_id, job_id)\n    request = ml.projects().jobs().get(name=job_name)\n\n    try:\n        response
          = request.execute()\n        print(response)   \n    except errors.HttpError
          as err:\n        print(err)\n    except:\n        print(''Unexpected error'')    \n\n    best_trial
          = response[''trainingOutput''][''trials''][0]\n\n    print(\"best_trial:\",
          best_trial)\n\n    metric_value = best_trial[''finalMetric''][''objectiveValue'']\n    learning_rate
          = float(best_trial[''hyperparameters''][''learning_rate''])\n    dropout_rate
          = float(best_trial[''hyperparameters''][''dropout_rate''])\n\n    return
          (metric_value, learning_rate, dropout_rate)\n\ndef _serialize_float(float_value:
          float) -> str:\n    if isinstance(float_value, str):\n        return float_value\n    if
          not isinstance(float_value, (float, int)):\n        raise TypeError(''Value
          \"{}\" has type \"{}\" instead of float.''.format(str(float_value), str(type(float_value))))\n    return
          str(float_value)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Retrieve
          best run'', description='''')\n_parser.add_argument(\"--project-id\", dest=\"project_id\",
          type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--job-id\",
          dest=\"job_id\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=3)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = retrieve_best_run(**_parsed_args)\n\n_output_serializers
          = [\n    _serialize_float,\n    _serialize_float,\n    _serialize_float,\n\n]\n\nimport
          os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "gcr.io/##########/docker_images/base_image:latest"}},
          "inputs": [{"name": "project_id", "type": "String"}, {"name": "job_id",
          "type": "String"}], "name": "Retrieve best run", "outputs": [{"name": "metric_value",
          "type": "Float"}, {"name": "learning_rate", "type": "Float"}, {"name": "dropout_rate",
          "type": "Float"}]}', pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"job_id":
          "{{inputs.parameters.submitting-a-cloud-ml-training-job-as-a-pipeline-step-job_id}}",
          "project_id": "{{inputs.parameters.project_id}}"}'}
  - name: run-transformation-pipeline
    container:
      args: [--source-table-name, '{{inputs.parameters.source_table_name}}', --job-name,
        preprocess-weather-features-210729-062335, --gcs-root, '{{inputs.parameters.gcs_root}}',
        --project-id, '{{inputs.parameters.project_id}}', --region, '{{inputs.parameters.region}}',
        --dataset-location, '{{inputs.parameters.gcs_root}}/datasets/{{workflow.uid}}',
        '----output-paths', /tmp/outputs/training_file_path/data, /tmp/outputs/validation_file_path/data,
        /tmp/outputs/testing_file_path/data, /tmp/outputs/transform_artefacts_dir/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def run_transformation_pipeline(\n    source_table_name, job_name, gcs_root,\
        \ project_id, region, dataset_location\n):\n\n    import copy\n    from datetime\
        \ import datetime\n    import math\n    import os\n    import tempfile\n\n\
        \    from jinja2 import Template\n    import apache_beam as beam\n\n    import\
        \ pandas as pd\n    import tensorflow as tf    \n    import tensorflow_transform\
        \ as tft\n    import tensorflow_transform.beam as tft_beam\n\n    # Setting\
        \ default value\n    NUMERICAL_FEATURES = [\n        'Date', 'air_pressure_ashore',\
        \ 'air_pressure_afloat', 'precipitation', 'temperature',\n        'humidity',\
        \ 'wind_direction', 'wind_velocity', 'hours_of_daylight', 'global_solar_radiation'\n\
        \    ]\n\n    RAW_DATA_FEATURE_SPEC = dict(\n        [(name, tf.io.FixedLenFeature([],\
        \ tf.float32)) for name in NUMERICAL_FEATURES]\n    )\n\n    raw_metadata\
        \ = tft.tf_metadata.dataset_metadata.DatasetMetadata(\n        tft.tf_metadata.schema_utils.schema_from_feature_spec(RAW_DATA_FEATURE_SPEC)\n\
        \    )    \n\n    # Generating the query\n    def generate_sampling_query(source_table_name,\
        \ step):\n        # Setting timestamp division\n        start = datetime(2011,\
        \ 1, 1, 1, 0, 0)\n        end = datetime.now()\n        diff = end.timestamp()\
        \ - start.timestamp()\n\n        train_start = start.timestamp()\n       \
        \ train_end = train_start + diff * 0.8\n        valid_end = train_end + diff\
        \ * 0.1\n        test_end = valid_end + diff * 0.1\n\n        train_start\
        \ = datetime.fromtimestamp(train_start)\n        train_end = datetime.fromtimestamp(train_end)\n\
        \        valid_end = datetime.fromtimestamp(valid_end)\n        test_end =\
        \ datetime.fromtimestamp(test_end)\n\n        valid_start = train_end\n  \
        \      test_start = valid_end\n\n        # Template query\n        sampling_query_template=\"\
        \"\"\n\n    SELECT\n        *\n    FROM \n        `{{source_table}}`\n   \
        \ WHERE\n        Date BETWEEN '{{start}}' AND '{{end}}'\n    ORDER BY \n \
        \       Date\n\n        \"\"\"\n\n        # Changing query dependging on steps\n\
        \        if step == \"Train\":\n            start, end = train_start, train_end\n\
        \        elif step == \"Valid\":\n            start, end = valid_start, valid_end\n\
        \        else:\n            start, end = test_start, test_end\n\n        query\
        \ = Template(sampling_query_template).render(\n            source_table=source_table_name,\
        \ start=start, end=end)\n\n        return query\n\n    def prep_bq_row(bq_row):\n\
        \n        result = {}\n\n        for feature_name in bq_row.keys():\n    \
        \        result[feature_name] = bq_row[feature_name]\n\n        date_time\
        \ = pd.to_datetime(bq_row[\"Date\"])\n        time_stamp = pd.Timestamp(date_time)\n\
        \        result[\"Date\"] = time_stamp.timestamp()\n\n        wind_direction\
        \ = tf.strings.regex_replace(bq_row[\"wind_direction\"], \"[\\s+)]\", \"\"\
        )\n        wind_direction = tf.strings.regex_replace(wind_direction, \"[x]\"\
        , u\"\u9759\u7A4F\")  \n\n        direction_list = [\n            \"\u5317\
        \", \"\u5317\u5317\u6771\", \"\u5317\u6771\", \"\u6771\u5317\u6771\", \"\u6771\
        \", \"\u6771\u5357\u6771\", \"\u5357\u6771\", \"\u5357\u5357\u6771\", \n \
        \           \"\u5357\", \"\u5357\u5357\u897F\", \"\u5357\u897F\", \"\u897F\
        \u5357\u897F\", \"\u897F\", \"\u897F\u5317\u897F\", \"\u5317\u897F\", \"\u5317\
        \u5317\u897F\", \"\u9759\u7A4F\"\n        ]\n        degree_list = [\n   \
        \         0.0, 22.5, 45.0, 67.5, 90.0, 112.5, 135.0, 157.5,\n            180.0,\
        \ 202.5, 225.0, 247.5, 270.0, 292.5, 315.0, 337.5, 0.0\n        ]\n\n    \
        \    def direction_to_degree(direction):\n            if direction in direction_list:\n\
        \                index = direction_list.index(direction)\n               \
        \ return degree_list[index]\n            else:\n                return 0.0\n\
        \n        result[\"wind_direction\"] = direction_to_degree(wind_direction)\n\
        \n        return result\n\n    def read_from_bq(pipeline, source_table_name,\
        \ step):\n\n        query = generate_sampling_query(source_table_name, step)\n\
        \n        # Read data from Bigquery\n        raw_data = (\n            pipeline\n\
        \            | 'Read{}DatafromBigQuery'.format(step) >> beam.io.Read(beam.io.ReadFromBigQuery(query=query,\
        \ use_standard_sql=True))\n            | 'Preproc{}Data'.format(step) >> beam.Map(prep_bq_row)\n\
        \        )\n\n        raw_dataset = (raw_data, raw_metadata)\n\n        return\
        \ raw_dataset\n\n    def preprocess_fn(inputs):    \n        outputs = {}\n\
        \n        # Date\n        timestamp_s = inputs[\"Date\"]\n\n        day =\
        \ 24 * 60 * 60\n        year = 365.2425 * day\n\n        outputs[\"day_sin\"\
        ] = tf.sin(timestamp_s * 2 * math.pi / day)\n        outputs[\"day_cos\"]\
        \ = tf.cos(timestamp_s * 2 * math.pi / day)\n\n        outputs[\"year_sin\"\
        ] = tf.sin(timestamp_s * 2 * math.pi / year)\n        outputs[\"year_cos\"\
        ] = tf.cos(timestamp_s * 2 * math.pi / year)\n\n        # Air pressure\n \
        \       STANDARDIZED_FEATURES_LIST = [\"air_pressure_ashore\", \"air_pressure_afloat\"\
        ]\n        for feature in STANDARDIZED_FEATURES_LIST:\n            outputs[feature]\
        \ = tft.scale_to_0_1(tf.clip_by_value(inputs[feature], 860.0, 1100.0))\n\n\
        \        outputs[\"diff_air_pressure\"] = outputs[\"air_pressure_ashore\"\
        ] - outputs[\"air_pressure_afloat\"] \n\n        # Wind\n        wind_direction_rad\
        \ = inputs[\"wind_direction\"] * math.pi / 180.0\n\n        outputs[\"wind_vector_x\"\
        ] = inputs[\"wind_velocity\"] * tf.cos(wind_direction_rad)\n        outputs[\"\
        wind_vector_y\"] = inputs[\"wind_velocity\"] * tf.sin(wind_direction_rad)\n\
        \n        # Others\n        # Normalizing numerical features\n        NORMALIZED_FEATURES_LIST\
        \ = [\"precipitation\", \"temperature\", \"humidity\", \"hours_of_daylight\"\
        , \"global_solar_radiation\"]\n        for feature in NORMALIZED_FEATURES_LIST:\n\
        \            outputs[feature] = tft.scale_to_z_score(inputs[feature])\n\n\
        \        # Calcurating stats of Temperature and Converting to feature\n  \
        \      def feature_from_scalar(value):\n            batch_size = tf.shape(input=inputs[\"\
        temperature\"])[0]\n\n            return tf.tile(tf.expand_dims(value, 0),\
        \ multiples=[batch_size])\n\n        outputs[\"temp_mean\"] = feature_from_scalar(tft.mean(inputs['temperature']))\n\
        \        outputs[\"temp_var\"] = feature_from_scalar(tft.var(inputs['temperature']))\n\
        \n        return outputs\n\n    def analyze_and_transform(raw_dataset, step):\
        \    \n\n        transformed_dataset, transform_fn = (\n            raw_dataset\n\
        \            | tft_beam.AnalyzeAndTransformDataset(preprocess_fn)\n      \
        \  )\n\n        return transformed_dataset, transform_fn\n\n    def transform(raw_dataset,\
        \ transform_fn, step):    \n\n        transformed_dataset = (\n          \
        \  (raw_dataset, transform_fn)\n            | '{}Transform'.format(step) >>\
        \ tft_beam.TransformDataset()\n        )\n\n        return transformed_dataset\n\
        \n    def to_train_csv(rawdata):   \n\n        TRAIN_CSV_COLUMNS = [\n   \
        \         'day_sin', 'day_cos', 'year_sin', 'year_cos', 'air_pressure_ashore',\
        \ 'air_pressure_afloat', 'diff_air_pressure',\n            'precipitation',\
        \ 'temperature', 'humidity', 'wind_vector_x', 'wind_vector_y',\n         \
        \   'hours_of_daylight', 'global_solar_radiation', 'temp_mean', 'temp_var'\n\
        \        ]\n\n        data = ','.join([str(rawdata[k]) for k in TRAIN_CSV_COLUMNS])\n\
        \n        yield str(data)\n\n    def to_test_csv(rawdata):\n\n        TEST_CSV_COLUMNS\
        \ = [\n            'Date', 'air_pressure_ashore', 'air_pressure_afloat', 'precipitation',\
        \ 'temperature',\n            'humidity', 'wind_direction', 'wind_velocity',\
        \ 'hours_of_daylight', 'global_solar_radiation'\n        ]\n\n        data\
        \ = ','.join([str(rawdata[k]) for k in TEST_CSV_COLUMNS])\n\n        yield\
        \ str(data)\n\n    def write_csv(transformed_dataset, location, step):   \
        \ \n\n        if step == \"Train\" or step == \"Valid\":\n            transformed_data,\
        \ _ = transformed_dataset\n            (\n                transformed_data\n\
        \                | '{}Csv'.format(step) >> beam.FlatMap(to_train_csv)\n  \
        \              | '{}Out'.format(step) >> beam.io.Write(beam.io.WriteToText(location))\n\
        \            )\n\n        else:\n            transformed_data, _ = transformed_dataset\n\
        \            (\n                transformed_data\n                | '{}Csv'.format(step)\
        \ >> beam.FlatMap(to_test_csv)\n                | '{}Out'.format(step) >>\
        \ beam.io.Write(beam.io.WriteToText(location))\n            )\n\n    def write_transform_artefacts(transform_fn,\
        \ location):\n        (\n            transform_fn\n            | 'WriteTransformArtefacts'\
        \ >> tft_beam.WriteTransformFn(location)\n\n        )\n\n    TRAINING_FILE_PATH\
        \ = 'training/data.csv'\n    VALIDATION_FILE_PATH = 'validation/data.csv'\n\
        \    TESTING_FILE_PATH = 'testing/data.csv'\n\n    options = {\n        'staging_location':\
        \ os.path.join(gcs_root, 'tmp', 'staging'),\n        'temp_location': os.path.join(gcs_root,\
        \ 'tmp'),\n        'job_name': job_name,\n        'project': project_id,\n\
        \        'max_num_workers': 3,\n        'save_main_session': True,\n     \
        \   'region': region,\n        'setup_file': './setup.py',\n        'teardown_policy':\
        \ 'TEARDOWN_ALWAYS',\n        'no_save_main_session': True,\n    }\n\n   \
        \ opts = beam.pipeline.PipelineOptions(flags=[], **options)\n\n    RUNNER\
        \ = 'DataflowRunner'\n\n    with beam.Pipeline(RUNNER, options=opts) as pipeline:\n\
        \        with tft_beam.Context(temp_dir=tempfile.mkdtemp()):\n\n         \
        \   # Create training set\n            step = \"Train\"            \n    \
        \        training_file_path = '{}/{}'.format(dataset_location, TRAINING_FILE_PATH)\n\
        \            tf_record_file_path = dataset_location\n\n            raw_train_dataset\
        \ = read_from_bq(pipeline, source_table_name, step)\n            transformed_train_dataset,\
        \ transform_fn = analyze_and_transform(raw_train_dataset, step)          \
        \  \n            write_csv(transformed_train_dataset, training_file_path,\
        \ step)\n\n            # Create validation set\n            step = \"Valid\"\
        \            \n            validation_file_path = '{}/{}'.format(dataset_location,\
        \ VALIDATION_FILE_PATH)\n\n            raw_eval_dataset = read_from_bq(pipeline,\
        \ source_table_name, step)\n            transformed_eval_dataset = transform(raw_eval_dataset,\
        \ transform_fn, step)\n            write_csv(transformed_eval_dataset, validation_file_path,\
        \ step)\n\n            # Create testing set\n            step = \"Test\"\n\
        \            testing_file_path = '{}/{}'.format(dataset_location, TESTING_FILE_PATH)\n\
        \n            raw_test_dataset = read_from_bq(pipeline, source_table_name,\
        \ step)\n            write_csv(raw_test_dataset, testing_file_path, step)\n\
        \n            # Sarving artefacts\n            transform_artefacts_dir = os.path.join(gcs_root,'transform')\
        \ \n            write_transform_artefacts(transform_fn, transform_artefacts_dir)\n\
        \n    return (training_file_path, validation_file_path, testing_file_path,\
        \ transform_artefacts_dir)\n\ndef _serialize_str(str_value: str) -> str:\n\
        \    if not isinstance(str_value, str):\n        raise TypeError('Value \"\
        {}\" has type \"{}\" instead of str.'.format(str(str_value), str(type(str_value))))\n\
        \    return str_value\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Run\
        \ transformation pipeline', description='')\n_parser.add_argument(\"--source-table-name\"\
        , dest=\"source_table_name\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--job-name\", dest=\"job_name\", type=str, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--gcs-root\", dest=\"\
        gcs_root\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --project-id\", dest=\"project_id\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--region\", dest=\"region\", type=str, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--dataset-location\"\
        , dest=\"dataset_location\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"----output-paths\", dest=\"_output_paths\", type=str,\
        \ nargs=4)\n_parsed_args = vars(_parser.parse_args())\n_output_files = _parsed_args.pop(\"\
        _output_paths\", [])\n\n_outputs = run_transformation_pipeline(**_parsed_args)\n\
        \n_output_serializers = [\n    _serialize_str,\n    _serialize_str,\n    _serialize_str,\n\
        \    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n\
        \    try:\n        os.makedirs(os.path.dirname(output_file))\n    except OSError:\n\
        \        pass\n    with open(output_file, 'w') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
      image: gcr.io/##########/docker_images/transform_image:latest
    inputs:
      parameters:
      - {name: gcs_root}
      - {name: project_id}
      - {name: region}
      - {name: source_table_name}
    outputs:
      parameters:
      - name: run-transformation-pipeline-testing_file_path
        valueFrom: {path: /tmp/outputs/testing_file_path/data}
      - name: run-transformation-pipeline-training_file_path
        valueFrom: {path: /tmp/outputs/training_file_path/data}
      - name: run-transformation-pipeline-transform_artefacts_dir
        valueFrom: {path: /tmp/outputs/transform_artefacts_dir/data}
      - name: run-transformation-pipeline-validation_file_path
        valueFrom: {path: /tmp/outputs/validation_file_path/data}
      artifacts:
      - {name: run-transformation-pipeline-testing_file_path, path: /tmp/outputs/testing_file_path/data}
      - {name: run-transformation-pipeline-training_file_path, path: /tmp/outputs/training_file_path/data}
      - {name: run-transformation-pipeline-transform_artefacts_dir, path: /tmp/outputs/transform_artefacts_dir/data}
      - {name: run-transformation-pipeline-validation_file_path, path: /tmp/outputs/validation_file_path/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.6.5
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--source-table-name", {"inputValue": "source_table_name"}, "--job-name",
          {"inputValue": "job_name"}, "--gcs-root", {"inputValue": "gcs_root"}, "--project-id",
          {"inputValue": "project_id"}, "--region", {"inputValue": "region"}, "--dataset-location",
          {"inputValue": "dataset_location"}, "----output-paths", {"outputPath": "training_file_path"},
          {"outputPath": "validation_file_path"}, {"outputPath": "testing_file_path"},
          {"outputPath": "transform_artefacts_dir"}], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def run_transformation_pipeline(\n    source_table_name, job_name, gcs_root,
          project_id, region, dataset_location\n):\n\n    import copy\n    from datetime
          import datetime\n    import math\n    import os\n    import tempfile\n\n    from
          jinja2 import Template\n    import apache_beam as beam\n\n    import pandas
          as pd\n    import tensorflow as tf    \n    import tensorflow_transform
          as tft\n    import tensorflow_transform.beam as tft_beam\n\n    # Setting
          default value\n    NUMERICAL_FEATURES = [\n        ''Date'', ''air_pressure_ashore'',
          ''air_pressure_afloat'', ''precipitation'', ''temperature'',\n        ''humidity'',
          ''wind_direction'', ''wind_velocity'', ''hours_of_daylight'', ''global_solar_radiation''\n    ]\n\n    RAW_DATA_FEATURE_SPEC
          = dict(\n        [(name, tf.io.FixedLenFeature([], tf.float32)) for name
          in NUMERICAL_FEATURES]\n    )\n\n    raw_metadata = tft.tf_metadata.dataset_metadata.DatasetMetadata(\n        tft.tf_metadata.schema_utils.schema_from_feature_spec(RAW_DATA_FEATURE_SPEC)\n    )    \n\n    #
          Generating the query\n    def generate_sampling_query(source_table_name,
          step):\n        # Setting timestamp division\n        start = datetime(2011,
          1, 1, 1, 0, 0)\n        end = datetime.now()\n        diff = end.timestamp()
          - start.timestamp()\n\n        train_start = start.timestamp()\n        train_end
          = train_start + diff * 0.8\n        valid_end = train_end + diff * 0.1\n        test_end
          = valid_end + diff * 0.1\n\n        train_start = datetime.fromtimestamp(train_start)\n        train_end
          = datetime.fromtimestamp(train_end)\n        valid_end = datetime.fromtimestamp(valid_end)\n        test_end
          = datetime.fromtimestamp(test_end)\n\n        valid_start = train_end\n        test_start
          = valid_end\n\n        # Template query\n        sampling_query_template=\"\"\"\n\n    SELECT\n        *\n    FROM
          \n        `{{source_table}}`\n    WHERE\n        Date BETWEEN ''{{start}}''
          AND ''{{end}}''\n    ORDER BY \n        Date\n\n        \"\"\"\n\n        #
          Changing query dependging on steps\n        if step == \"Train\":\n            start,
          end = train_start, train_end\n        elif step == \"Valid\":\n            start,
          end = valid_start, valid_end\n        else:\n            start, end = test_start,
          test_end\n\n        query = Template(sampling_query_template).render(\n            source_table=source_table_name,
          start=start, end=end)\n\n        return query\n\n    def prep_bq_row(bq_row):\n\n        result
          = {}\n\n        for feature_name in bq_row.keys():\n            result[feature_name]
          = bq_row[feature_name]\n\n        date_time = pd.to_datetime(bq_row[\"Date\"])\n        time_stamp
          = pd.Timestamp(date_time)\n        result[\"Date\"] = time_stamp.timestamp()\n\n        wind_direction
          = tf.strings.regex_replace(bq_row[\"wind_direction\"], \"[\\s+)]\", \"\")\n        wind_direction
          = tf.strings.regex_replace(wind_direction, \"[x]\", u\"\u9759\u7a4f\")  \n\n        direction_list
          = [\n            \"\u5317\", \"\u5317\u5317\u6771\", \"\u5317\u6771\", \"\u6771\u5317\u6771\",
          \"\u6771\", \"\u6771\u5357\u6771\", \"\u5357\u6771\", \"\u5357\u5357\u6771\",
          \n            \"\u5357\", \"\u5357\u5357\u897f\", \"\u5357\u897f\", \"\u897f\u5357\u897f\",
          \"\u897f\", \"\u897f\u5317\u897f\", \"\u5317\u897f\", \"\u5317\u5317\u897f\",
          \"\u9759\u7a4f\"\n        ]\n        degree_list = [\n            0.0, 22.5,
          45.0, 67.5, 90.0, 112.5, 135.0, 157.5,\n            180.0, 202.5, 225.0,
          247.5, 270.0, 292.5, 315.0, 337.5, 0.0\n        ]\n\n        def direction_to_degree(direction):\n            if
          direction in direction_list:\n                index = direction_list.index(direction)\n                return
          degree_list[index]\n            else:\n                return 0.0\n\n        result[\"wind_direction\"]
          = direction_to_degree(wind_direction)\n\n        return result\n\n    def
          read_from_bq(pipeline, source_table_name, step):\n\n        query = generate_sampling_query(source_table_name,
          step)\n\n        # Read data from Bigquery\n        raw_data = (\n            pipeline\n            |
          ''Read{}DatafromBigQuery''.format(step) >> beam.io.Read(beam.io.ReadFromBigQuery(query=query,
          use_standard_sql=True))\n            | ''Preproc{}Data''.format(step) >>
          beam.Map(prep_bq_row)\n        )\n\n        raw_dataset = (raw_data, raw_metadata)\n\n        return
          raw_dataset\n\n    def preprocess_fn(inputs):    \n        outputs = {}\n\n        #
          Date\n        timestamp_s = inputs[\"Date\"]\n\n        day = 24 * 60 *
          60\n        year = 365.2425 * day\n\n        outputs[\"day_sin\"] = tf.sin(timestamp_s
          * 2 * math.pi / day)\n        outputs[\"day_cos\"] = tf.cos(timestamp_s
          * 2 * math.pi / day)\n\n        outputs[\"year_sin\"] = tf.sin(timestamp_s
          * 2 * math.pi / year)\n        outputs[\"year_cos\"] = tf.cos(timestamp_s
          * 2 * math.pi / year)\n\n        # Air pressure\n        STANDARDIZED_FEATURES_LIST
          = [\"air_pressure_ashore\", \"air_pressure_afloat\"]\n        for feature
          in STANDARDIZED_FEATURES_LIST:\n            outputs[feature] = tft.scale_to_0_1(tf.clip_by_value(inputs[feature],
          860.0, 1100.0))\n\n        outputs[\"diff_air_pressure\"] = outputs[\"air_pressure_ashore\"]
          - outputs[\"air_pressure_afloat\"] \n\n        # Wind\n        wind_direction_rad
          = inputs[\"wind_direction\"] * math.pi / 180.0\n\n        outputs[\"wind_vector_x\"]
          = inputs[\"wind_velocity\"] * tf.cos(wind_direction_rad)\n        outputs[\"wind_vector_y\"]
          = inputs[\"wind_velocity\"] * tf.sin(wind_direction_rad)\n\n        # Others\n        #
          Normalizing numerical features\n        NORMALIZED_FEATURES_LIST = [\"precipitation\",
          \"temperature\", \"humidity\", \"hours_of_daylight\", \"global_solar_radiation\"]\n        for
          feature in NORMALIZED_FEATURES_LIST:\n            outputs[feature] = tft.scale_to_z_score(inputs[feature])\n\n        #
          Calcurating stats of Temperature and Converting to feature\n        def
          feature_from_scalar(value):\n            batch_size = tf.shape(input=inputs[\"temperature\"])[0]\n\n            return
          tf.tile(tf.expand_dims(value, 0), multiples=[batch_size])\n\n        outputs[\"temp_mean\"]
          = feature_from_scalar(tft.mean(inputs[''temperature'']))\n        outputs[\"temp_var\"]
          = feature_from_scalar(tft.var(inputs[''temperature'']))\n\n        return
          outputs\n\n    def analyze_and_transform(raw_dataset, step):    \n\n        transformed_dataset,
          transform_fn = (\n            raw_dataset\n            | tft_beam.AnalyzeAndTransformDataset(preprocess_fn)\n        )\n\n        return
          transformed_dataset, transform_fn\n\n    def transform(raw_dataset, transform_fn,
          step):    \n\n        transformed_dataset = (\n            (raw_dataset,
          transform_fn)\n            | ''{}Transform''.format(step) >> tft_beam.TransformDataset()\n        )\n\n        return
          transformed_dataset\n\n    def to_train_csv(rawdata):   \n\n        TRAIN_CSV_COLUMNS
          = [\n            ''day_sin'', ''day_cos'', ''year_sin'', ''year_cos'', ''air_pressure_ashore'',
          ''air_pressure_afloat'', ''diff_air_pressure'',\n            ''precipitation'',
          ''temperature'', ''humidity'', ''wind_vector_x'', ''wind_vector_y'',\n            ''hours_of_daylight'',
          ''global_solar_radiation'', ''temp_mean'', ''temp_var''\n        ]\n\n        data
          = '',''.join([str(rawdata[k]) for k in TRAIN_CSV_COLUMNS])\n\n        yield
          str(data)\n\n    def to_test_csv(rawdata):\n\n        TEST_CSV_COLUMNS =
          [\n            ''Date'', ''air_pressure_ashore'', ''air_pressure_afloat'',
          ''precipitation'', ''temperature'',\n            ''humidity'', ''wind_direction'',
          ''wind_velocity'', ''hours_of_daylight'', ''global_solar_radiation''\n        ]\n\n        data
          = '',''.join([str(rawdata[k]) for k in TEST_CSV_COLUMNS])\n\n        yield
          str(data)\n\n    def write_csv(transformed_dataset, location, step):    \n\n        if
          step == \"Train\" or step == \"Valid\":\n            transformed_data, _
          = transformed_dataset\n            (\n                transformed_data\n                |
          ''{}Csv''.format(step) >> beam.FlatMap(to_train_csv)\n                |
          ''{}Out''.format(step) >> beam.io.Write(beam.io.WriteToText(location))\n            )\n\n        else:\n            transformed_data,
          _ = transformed_dataset\n            (\n                transformed_data\n                |
          ''{}Csv''.format(step) >> beam.FlatMap(to_test_csv)\n                | ''{}Out''.format(step)
          >> beam.io.Write(beam.io.WriteToText(location))\n            )\n\n    def
          write_transform_artefacts(transform_fn, location):\n        (\n            transform_fn\n            |
          ''WriteTransformArtefacts'' >> tft_beam.WriteTransformFn(location)\n\n        )\n\n    TRAINING_FILE_PATH
          = ''training/data.csv''\n    VALIDATION_FILE_PATH = ''validation/data.csv''\n    TESTING_FILE_PATH
          = ''testing/data.csv''\n\n    options = {\n        ''staging_location'':
          os.path.join(gcs_root, ''tmp'', ''staging''),\n        ''temp_location'':
          os.path.join(gcs_root, ''tmp''),\n        ''job_name'': job_name,\n        ''project'':
          project_id,\n        ''max_num_workers'': 3,\n        ''save_main_session'':
          True,\n        ''region'': region,\n        ''setup_file'': ''./setup.py'',\n        ''teardown_policy'':
          ''TEARDOWN_ALWAYS'',\n        ''no_save_main_session'': True,\n    }\n\n    opts
          = beam.pipeline.PipelineOptions(flags=[], **options)\n\n    RUNNER = ''DataflowRunner''\n\n    with
          beam.Pipeline(RUNNER, options=opts) as pipeline:\n        with tft_beam.Context(temp_dir=tempfile.mkdtemp()):\n\n            #
          Create training set\n            step = \"Train\"            \n            training_file_path
          = ''{}/{}''.format(dataset_location, TRAINING_FILE_PATH)\n            tf_record_file_path
          = dataset_location\n\n            raw_train_dataset = read_from_bq(pipeline,
          source_table_name, step)\n            transformed_train_dataset, transform_fn
          = analyze_and_transform(raw_train_dataset, step)            \n            write_csv(transformed_train_dataset,
          training_file_path, step)\n\n            # Create validation set\n            step
          = \"Valid\"            \n            validation_file_path = ''{}/{}''.format(dataset_location,
          VALIDATION_FILE_PATH)\n\n            raw_eval_dataset = read_from_bq(pipeline,
          source_table_name, step)\n            transformed_eval_dataset = transform(raw_eval_dataset,
          transform_fn, step)\n            write_csv(transformed_eval_dataset, validation_file_path,
          step)\n\n            # Create testing set\n            step = \"Test\"\n            testing_file_path
          = ''{}/{}''.format(dataset_location, TESTING_FILE_PATH)\n\n            raw_test_dataset
          = read_from_bq(pipeline, source_table_name, step)\n            write_csv(raw_test_dataset,
          testing_file_path, step)\n\n            # Sarving artefacts\n            transform_artefacts_dir
          = os.path.join(gcs_root,''transform'') \n            write_transform_artefacts(transform_fn,
          transform_artefacts_dir)\n\n    return (training_file_path, validation_file_path,
          testing_file_path, transform_artefacts_dir)\n\ndef _serialize_str(str_value:
          str) -> str:\n    if not isinstance(str_value, str):\n        raise TypeError(''Value
          \"{}\" has type \"{}\" instead of str.''.format(str(str_value), str(type(str_value))))\n    return
          str_value\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Run
          transformation pipeline'', description='''')\n_parser.add_argument(\"--source-table-name\",
          dest=\"source_table_name\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--job-name\",
          dest=\"job_name\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--gcs-root\",
          dest=\"gcs_root\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--project-id\",
          dest=\"project_id\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--region\",
          dest=\"region\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dataset-location\",
          dest=\"dataset_location\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=4)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = run_transformation_pipeline(**_parsed_args)\n\n_output_serializers
          = [\n    _serialize_str,\n    _serialize_str,\n    _serialize_str,\n    _serialize_str,\n\n]\n\nimport
          os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "gcr.io/##########/docker_images/transform_image:latest"}},
          "inputs": [{"name": "source_table_name", "type": "String"}, {"name": "job_name",
          "type": "String"}, {"name": "gcs_root", "type": "String"}, {"name": "project_id",
          "type": "String"}, {"name": "region", "type": "String"}, {"name": "dataset_location",
          "type": "String"}], "name": "Run transformation pipeline", "outputs": [{"name":
          "training_file_path", "type": "String"}, {"name": "validation_file_path",
          "type": "String"}, {"name": "testing_file_path", "type": "String"}, {"name":
          "transform_artefacts_dir", "type": "String"}]}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"dataset_location": "{{inputs.parameters.gcs_root}}/datasets/{{workflow.uid}}",
          "gcs_root": "{{inputs.parameters.gcs_root}}", "job_name": "preprocess-weather-features-210729-062335",
          "project_id": "{{inputs.parameters.project_id}}", "region": "{{inputs.parameters.region}}",
          "source_table_name": "{{inputs.parameters.source_table_name}}"}'}
  - name: submitting-a-cloud-ml-training-job-as-a-pipeline-step
    container:
      args: [--ui_metadata_path, /tmp/outputs/MLPipeline_UI_metadata/data, kfp_component.google.ml_engine,
        train, --project_id, '{{inputs.parameters.project_id}}', --python_module,
        '', --package_uris, '', --region, '{{inputs.parameters.region}}', --args,
        '["--training_dataset_path", "{{inputs.parameters.run-transformation-pipeline-training_file_path}}",
          "--validation_dataset_path", "{{inputs.parameters.run-transformation-pipeline-validation_file_path}}",
          "--num_epochs", "{{inputs.parameters.num_epochs_hypertune}}", "--num_units",
          "{{inputs.parameters.num_units}}", "--hptune", "True", "--transform_artefacts_dir",
          "{{inputs.parameters.run-transformation-pipeline-transform_artefacts_dir}}"]',
        --job_dir, '{{inputs.parameters.gcs_root}}/jobdir/hypertune/{{workflow.uid}}',
        --python_version, '', --runtime_version, '', --master_image_uri, 'gcr.io/##########/docker_images/trainer_image:latest',
        --worker_image_uri, '', --training_input, '{{inputs.parameters.hypertune_settings}}',
        --job_id_prefix, '', --job_id, '', --wait_interval, '30', --job_id_output_path,
        /tmp/outputs/job_id/data, --job_dir_output_path, /tmp/outputs/job_dir/data]
      command: []
      env:
      - {name: KFP_POD_NAME, value: '{{pod.name}}'}
      - {name: GOOGLE_APPLICATION_CREDENTIALS, value: /secret/gcp-credentials/user-gcp-sa.json}
      - {name: CLOUDSDK_AUTH_CREDENTIAL_FILE_OVERRIDE, value: /secret/gcp-credentials/user-gcp-sa.json}
      - name: KFP_POD_NAME
        valueFrom:
          fieldRef: {fieldPath: metadata.name}
      - name: KFP_NAMESPACE
        valueFrom:
          fieldRef: {fieldPath: metadata.namespace}
      - name: WORKFLOW_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''workflows.argoproj.io/workflow'']'}
      - name: ENABLE_CACHING
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipelines.kubeflow.org/enable_caching'']'}
      image: gcr.io/ml-pipeline/ml-pipeline-gcp:1.6.0
      volumeMounts:
      - {mountPath: /secret/gcp-credentials, name: gcp-credentials-user-gcp-sa}
    inputs:
      parameters:
      - {name: gcs_root}
      - {name: hypertune_settings}
      - {name: num_epochs_hypertune}
      - {name: num_units}
      - {name: project_id}
      - {name: region}
      - {name: run-transformation-pipeline-training_file_path}
      - {name: run-transformation-pipeline-transform_artefacts_dir}
      - {name: run-transformation-pipeline-validation_file_path}
    outputs:
      parameters:
      - name: submitting-a-cloud-ml-training-job-as-a-pipeline-step-job_id
        valueFrom: {path: /tmp/outputs/job_id/data}
      artifacts:
      - {name: mlpipeline-ui-metadata, path: /tmp/outputs/MLPipeline_UI_metadata/data}
      - {name: submitting-a-cloud-ml-training-job-as-a-pipeline-step-job_dir, path: /tmp/outputs/job_dir/data}
      - {name: submitting-a-cloud-ml-training-job-as-a-pipeline-step-job_id, path: /tmp/outputs/job_id/data}
    metadata:
      labels:
        add-pod-env: "true"
        pipelines.kubeflow.org/kfp_sdk_version: 1.6.5
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "A Kubeflow
          Pipeline component to submit a Cloud Machine Learning (Cloud ML) \nEngine
          training job as a step in a pipeline.\n", "implementation": {"container":
          {"args": ["--ui_metadata_path", {"outputPath": "MLPipeline UI metadata"},
          "kfp_component.google.ml_engine", "train", "--project_id", {"inputValue":
          "project_id"}, "--python_module", {"inputValue": "python_module"}, "--package_uris",
          {"inputValue": "package_uris"}, "--region", {"inputValue": "region"}, "--args",
          {"inputValue": "args"}, "--job_dir", {"inputValue": "job_dir"}, "--python_version",
          {"inputValue": "python_version"}, "--runtime_version", {"inputValue": "runtime_version"},
          "--master_image_uri", {"inputValue": "master_image_uri"}, "--worker_image_uri",
          {"inputValue": "worker_image_uri"}, "--training_input", {"inputValue": "training_input"},
          "--job_id_prefix", {"inputValue": "job_id_prefix"}, "--job_id", {"inputValue":
          "job_id"}, "--wait_interval", {"inputValue": "wait_interval"}, "--job_id_output_path",
          {"outputPath": "job_id"}, "--job_dir_output_path", {"outputPath": "job_dir"}],
          "env": {"KFP_POD_NAME": "{{pod.name}}"}, "image": "gcr.io/ml-pipeline/ml-pipeline-gcp:1.6.0"}},
          "inputs": [{"description": "Required. The ID of the parent project of the
          job.", "name": "project_id", "type": "GCPProjectID"}, {"default": "", "description":
          "The Python module name to run after installing the packages.", "name":
          "python_module", "type": "String"}, {"default": "", "description": "The
          Cloud Storage location of the packages (that contain the training program  and
          any additional dependencies). The maximum number of package URIs is 100.",
          "name": "package_uris", "type": "List"}, {"default": "", "description":
          "The Compute Engine region in which the training job is run.", "name": "region",
          "type": "GCPRegion"}, {"default": "", "description": "The command line arguments
          to pass to the program.", "name": "args", "type": "List"}, {"default": "",
          "description": "A Cloud Storage path in which to store the training outputs
          and other data  needed for training. This path is passed to your TensorFlow
          program as the  `job-dir` command-line argument. The benefit of specifying
          this field is  that Cloud ML validates the path for use in training.", "name":
          "job_dir", "type": "GCSPath"}, {"default": "", "description": "The version
          of Python used in training. If not set, the default version is `2.7`. Python
          `3.5` is available when runtimeVersion is set to `1.4` and above.", "name":
          "python_version", "type": "String"}, {"default": "", "description": "The
          Cloud ML Engine runtime version to use for training. If not set, Cloud ML
          Engine uses the default stable version, 1.0.", "name": "runtime_version",
          "type": "String"}, {"default": "", "description": "The Docker image to run
          on the master replica. This image must be in Container Registry.", "name":
          "master_image_uri", "type": "GCRPath"}, {"default": "", "description": "The
          Docker image to run on the worker replica. This image must be in Container
          Registry.", "name": "worker_image_uri", "type": "GCRPath"}, {"default":
          "", "description": "The input parameters to create a training job. It is
          the JSON payload  of a [TrainingInput](https://cloud.google.com/ml-engine/reference/rest/v1/projects.jobs#TrainingInput)",
          "name": "training_input", "type": "Dict"}, {"default": "", "description":
          "The prefix of the generated job id.", "name": "job_id_prefix", "type":
          "String"}, {"default": "", "description": "The ID of the job to create,
          takes precedence over generated job id if set.", "name": "job_id", "type":
          "String"}, {"default": "30", "description": "Optional. A time-interval to
          wait for between calls to get the job status.  Defaults to 30.''", "name":
          "wait_interval", "type": "Integer"}], "metadata": {"labels": {"add-pod-env":
          "true"}}, "name": "Submitting a Cloud ML training job as a pipeline step",
          "outputs": [{"description": "The ID of the created job.", "name": "job_id",
          "type": "String"}, {"description": "The output path in Cloud Storage of
          the training job, which contains the trained model files.", "name": "job_dir",
          "type": "GCSPath"}, {"name": "MLPipeline UI metadata", "type": "UI metadata"}]}',
        pipelines.kubeflow.org/component_ref: '{"digest": "fe13148fabaa6e5244a28173c8d38ff084452039113522de6ffc1fdeb22bb963",
          "name": "ml_engine/train", "url": "https://raw.githubusercontent.com/kubeflow/pipelines/1.6.0/components/gcp/ml_engine/train/component.yaml"}',
        pipelines.kubeflow.org/arguments.parameters: '{"args": "[\"--training_dataset_path\",
          \"{{inputs.parameters.run-transformation-pipeline-training_file_path}}\",
          \"--validation_dataset_path\", \"{{inputs.parameters.run-transformation-pipeline-validation_file_path}}\",
          \"--num_epochs\", \"{{inputs.parameters.num_epochs_hypertune}}\", \"--num_units\",
          \"{{inputs.parameters.num_units}}\", \"--hptune\", \"True\", \"--transform_artefacts_dir\",
          \"{{inputs.parameters.run-transformation-pipeline-transform_artefacts_dir}}\"]",
          "job_dir": "{{inputs.parameters.gcs_root}}/jobdir/hypertune/{{workflow.uid}}",
          "job_id": "", "job_id_prefix": "", "master_image_uri": "gcr.io/##########/docker_images/trainer_image:latest",
          "package_uris": "", "project_id": "{{inputs.parameters.project_id}}", "python_module":
          "", "python_version": "", "region": "{{inputs.parameters.region}}", "runtime_version":
          "", "training_input": "{{inputs.parameters.hypertune_settings}}", "wait_interval":
          "30", "worker_image_uri": ""}'}
    volumes:
    - name: gcp-credentials-user-gcp-sa
      secret: {secretName: user-gcp-sa}
  - name: submitting-a-cloud-ml-training-job-as-a-pipeline-step-2
    container:
      args: [--ui_metadata_path, /tmp/outputs/MLPipeline_UI_metadata/data, kfp_component.google.ml_engine,
        train, --project_id, '{{inputs.parameters.project_id}}', --python_module,
        '', --package_uris, '', --region, '{{inputs.parameters.region}}', --args,
        '["--training_dataset_path", "{{inputs.parameters.run-transformation-pipeline-training_file_path}}",
          "--validation_dataset_path", "{{inputs.parameters.run-transformation-pipeline-validation_file_path}}",
          "--num_epochs", "{{inputs.parameters.num_epochs_retrain}}", "--num_units",
          "{{inputs.parameters.num_units}}", "--learning_rate", "{{inputs.parameters.retrieve-best-run-learning_rate}}",
          "--dropout_rate", "{{inputs.parameters.retrieve-best-run-dropout_rate}}",
          "--hptune", "False", "--transform_artefacts_dir", "{{inputs.parameters.run-transformation-pipeline-transform_artefacts_dir}}"]',
        --job_dir, '{{inputs.parameters.gcs_root}}/jobdir/{{workflow.uid}}', --python_version,
        '', --runtime_version, '', --master_image_uri, 'gcr.io/##########/docker_images/trainer_image:latest',
        --worker_image_uri, '', --training_input, '', --job_id_prefix, '', --job_id,
        '', --wait_interval, '30', --job_id_output_path, /tmp/outputs/job_id/data,
        --job_dir_output_path, /tmp/outputs/job_dir/data]
      command: []
      env:
      - {name: KFP_POD_NAME, value: '{{pod.name}}'}
      - {name: GOOGLE_APPLICATION_CREDENTIALS, value: /secret/gcp-credentials/user-gcp-sa.json}
      - {name: CLOUDSDK_AUTH_CREDENTIAL_FILE_OVERRIDE, value: /secret/gcp-credentials/user-gcp-sa.json}
      - name: KFP_POD_NAME
        valueFrom:
          fieldRef: {fieldPath: metadata.name}
      - name: KFP_NAMESPACE
        valueFrom:
          fieldRef: {fieldPath: metadata.namespace}
      - name: WORKFLOW_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''workflows.argoproj.io/workflow'']'}
      - name: ENABLE_CACHING
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipelines.kubeflow.org/enable_caching'']'}
      image: gcr.io/ml-pipeline/ml-pipeline-gcp:1.6.0
      volumeMounts:
      - {mountPath: /secret/gcp-credentials, name: gcp-credentials-user-gcp-sa}
    inputs:
      parameters:
      - {name: gcs_root}
      - {name: num_epochs_retrain}
      - {name: num_units}
      - {name: project_id}
      - {name: region}
      - {name: retrieve-best-run-dropout_rate}
      - {name: retrieve-best-run-learning_rate}
      - {name: run-transformation-pipeline-training_file_path}
      - {name: run-transformation-pipeline-transform_artefacts_dir}
      - {name: run-transformation-pipeline-validation_file_path}
    outputs:
      parameters:
      - name: submitting-a-cloud-ml-training-job-as-a-pipeline-step-2-job_dir
        valueFrom: {path: /tmp/outputs/job_dir/data}
      artifacts:
      - {name: mlpipeline-ui-metadata, path: /tmp/outputs/MLPipeline_UI_metadata/data}
      - {name: submitting-a-cloud-ml-training-job-as-a-pipeline-step-2-job_dir, path: /tmp/outputs/job_dir/data}
      - {name: submitting-a-cloud-ml-training-job-as-a-pipeline-step-2-job_id, path: /tmp/outputs/job_id/data}
    metadata:
      labels:
        add-pod-env: "true"
        pipelines.kubeflow.org/kfp_sdk_version: 1.6.5
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "A Kubeflow
          Pipeline component to submit a Cloud Machine Learning (Cloud ML) \nEngine
          training job as a step in a pipeline.\n", "implementation": {"container":
          {"args": ["--ui_metadata_path", {"outputPath": "MLPipeline UI metadata"},
          "kfp_component.google.ml_engine", "train", "--project_id", {"inputValue":
          "project_id"}, "--python_module", {"inputValue": "python_module"}, "--package_uris",
          {"inputValue": "package_uris"}, "--region", {"inputValue": "region"}, "--args",
          {"inputValue": "args"}, "--job_dir", {"inputValue": "job_dir"}, "--python_version",
          {"inputValue": "python_version"}, "--runtime_version", {"inputValue": "runtime_version"},
          "--master_image_uri", {"inputValue": "master_image_uri"}, "--worker_image_uri",
          {"inputValue": "worker_image_uri"}, "--training_input", {"inputValue": "training_input"},
          "--job_id_prefix", {"inputValue": "job_id_prefix"}, "--job_id", {"inputValue":
          "job_id"}, "--wait_interval", {"inputValue": "wait_interval"}, "--job_id_output_path",
          {"outputPath": "job_id"}, "--job_dir_output_path", {"outputPath": "job_dir"}],
          "env": {"KFP_POD_NAME": "{{pod.name}}"}, "image": "gcr.io/ml-pipeline/ml-pipeline-gcp:1.6.0"}},
          "inputs": [{"description": "Required. The ID of the parent project of the
          job.", "name": "project_id", "type": "GCPProjectID"}, {"default": "", "description":
          "The Python module name to run after installing the packages.", "name":
          "python_module", "type": "String"}, {"default": "", "description": "The
          Cloud Storage location of the packages (that contain the training program  and
          any additional dependencies). The maximum number of package URIs is 100.",
          "name": "package_uris", "type": "List"}, {"default": "", "description":
          "The Compute Engine region in which the training job is run.", "name": "region",
          "type": "GCPRegion"}, {"default": "", "description": "The command line arguments
          to pass to the program.", "name": "args", "type": "List"}, {"default": "",
          "description": "A Cloud Storage path in which to store the training outputs
          and other data  needed for training. This path is passed to your TensorFlow
          program as the  `job-dir` command-line argument. The benefit of specifying
          this field is  that Cloud ML validates the path for use in training.", "name":
          "job_dir", "type": "GCSPath"}, {"default": "", "description": "The version
          of Python used in training. If not set, the default version is `2.7`. Python
          `3.5` is available when runtimeVersion is set to `1.4` and above.", "name":
          "python_version", "type": "String"}, {"default": "", "description": "The
          Cloud ML Engine runtime version to use for training. If not set, Cloud ML
          Engine uses the default stable version, 1.0.", "name": "runtime_version",
          "type": "String"}, {"default": "", "description": "The Docker image to run
          on the master replica. This image must be in Container Registry.", "name":
          "master_image_uri", "type": "GCRPath"}, {"default": "", "description": "The
          Docker image to run on the worker replica. This image must be in Container
          Registry.", "name": "worker_image_uri", "type": "GCRPath"}, {"default":
          "", "description": "The input parameters to create a training job. It is
          the JSON payload  of a [TrainingInput](https://cloud.google.com/ml-engine/reference/rest/v1/projects.jobs#TrainingInput)",
          "name": "training_input", "type": "Dict"}, {"default": "", "description":
          "The prefix of the generated job id.", "name": "job_id_prefix", "type":
          "String"}, {"default": "", "description": "The ID of the job to create,
          takes precedence over generated job id if set.", "name": "job_id", "type":
          "String"}, {"default": "30", "description": "Optional. A time-interval to
          wait for between calls to get the job status.  Defaults to 30.''", "name":
          "wait_interval", "type": "Integer"}], "metadata": {"labels": {"add-pod-env":
          "true"}}, "name": "Submitting a Cloud ML training job as a pipeline step",
          "outputs": [{"description": "The ID of the created job.", "name": "job_id",
          "type": "String"}, {"description": "The output path in Cloud Storage of
          the training job, which contains the trained model files.", "name": "job_dir",
          "type": "GCSPath"}, {"name": "MLPipeline UI metadata", "type": "UI metadata"}]}',
        pipelines.kubeflow.org/component_ref: '{"digest": "fe13148fabaa6e5244a28173c8d38ff084452039113522de6ffc1fdeb22bb963",
          "name": "ml_engine/train", "url": "https://raw.githubusercontent.com/kubeflow/pipelines/1.6.0/components/gcp/ml_engine/train/component.yaml"}',
        pipelines.kubeflow.org/arguments.parameters: '{"args": "[\"--training_dataset_path\",
          \"{{inputs.parameters.run-transformation-pipeline-training_file_path}}\",
          \"--validation_dataset_path\", \"{{inputs.parameters.run-transformation-pipeline-validation_file_path}}\",
          \"--num_epochs\", \"{{inputs.parameters.num_epochs_retrain}}\", \"--num_units\",
          \"{{inputs.parameters.num_units}}\", \"--learning_rate\", \"{{inputs.parameters.retrieve-best-run-learning_rate}}\",
          \"--dropout_rate\", \"{{inputs.parameters.retrieve-best-run-dropout_rate}}\",
          \"--hptune\", \"False\", \"--transform_artefacts_dir\", \"{{inputs.parameters.run-transformation-pipeline-transform_artefacts_dir}}\"]",
          "job_dir": "{{inputs.parameters.gcs_root}}/jobdir/{{workflow.uid}}", "job_id":
          "", "job_id_prefix": "", "master_image_uri": "gcr.io/###########/docker_images/trainer_image:latest",
          "package_uris": "", "project_id": "{{inputs.parameters.project_id}}", "python_module":
          "", "python_version": "", "region": "{{inputs.parameters.region}}", "runtime_version":
          "", "training_input": "", "wait_interval": "30", "worker_image_uri": ""}'}
    volumes:
    - name: gcp-credentials-user-gcp-sa
      secret: {secretName: user-gcp-sa}
  - name: weather-forecast-model-training
    inputs:
      parameters:
      - {name: evaluation_metric_name}
      - {name: evaluation_metric_threshold}
      - {name: gcs_root}
      - {name: hypertune_settings}
      - {name: model_id}
      - {name: num_epochs_hypertune}
      - {name: num_epochs_retrain}
      - {name: num_units}
      - {name: project_id}
      - {name: region}
      - {name: replace_existing_version}
      - {name: source_table_name}
      - {name: version_id}
    dag:
      tasks:
      - name: condition-1
        template: condition-1
        when: '{{tasks.evaluate-model.outputs.parameters.evaluate-model-metric_value}}
          < {{inputs.parameters.evaluation_metric_threshold}}'
        dependencies: [evaluate-model, submitting-a-cloud-ml-training-job-as-a-pipeline-step-2]
        arguments:
          parameters:
          - {name: model_id, value: '{{inputs.parameters.model_id}}'}
          - {name: project_id, value: '{{inputs.parameters.project_id}}'}
          - {name: region, value: '{{inputs.parameters.region}}'}
          - {name: replace_existing_version, value: '{{inputs.parameters.replace_existing_version}}'}
          - {name: submitting-a-cloud-ml-training-job-as-a-pipeline-step-2-job_dir,
            value: '{{tasks.submitting-a-cloud-ml-training-job-as-a-pipeline-step-2.outputs.parameters.submitting-a-cloud-ml-training-job-as-a-pipeline-step-2-job_dir}}'}
          - {name: version_id, value: '{{inputs.parameters.version_id}}'}
      - name: evaluate-model
        template: evaluate-model
        dependencies: [run-transformation-pipeline, submitting-a-cloud-ml-training-job-as-a-pipeline-step-2]
        arguments:
          parameters:
          - {name: evaluation_metric_name, value: '{{inputs.parameters.evaluation_metric_name}}'}
          - {name: run-transformation-pipeline-testing_file_path, value: '{{tasks.run-transformation-pipeline.outputs.parameters.run-transformation-pipeline-testing_file_path}}'}
          - {name: run-transformation-pipeline-transform_artefacts_dir, value: '{{tasks.run-transformation-pipeline.outputs.parameters.run-transformation-pipeline-transform_artefacts_dir}}'}
          - {name: submitting-a-cloud-ml-training-job-as-a-pipeline-step-2-job_dir,
            value: '{{tasks.submitting-a-cloud-ml-training-job-as-a-pipeline-step-2.outputs.parameters.submitting-a-cloud-ml-training-job-as-a-pipeline-step-2-job_dir}}'}
      - name: retrieve-best-run
        template: retrieve-best-run
        dependencies: [submitting-a-cloud-ml-training-job-as-a-pipeline-step]
        arguments:
          parameters:
          - {name: project_id, value: '{{inputs.parameters.project_id}}'}
          - {name: submitting-a-cloud-ml-training-job-as-a-pipeline-step-job_id, value: '{{tasks.submitting-a-cloud-ml-training-job-as-a-pipeline-step.outputs.parameters.submitting-a-cloud-ml-training-job-as-a-pipeline-step-job_id}}'}
      - name: run-transformation-pipeline
        template: run-transformation-pipeline
        arguments:
          parameters:
          - {name: gcs_root, value: '{{inputs.parameters.gcs_root}}'}
          - {name: project_id, value: '{{inputs.parameters.project_id}}'}
          - {name: region, value: '{{inputs.parameters.region}}'}
          - {name: source_table_name, value: '{{inputs.parameters.source_table_name}}'}
      - name: submitting-a-cloud-ml-training-job-as-a-pipeline-step
        template: submitting-a-cloud-ml-training-job-as-a-pipeline-step
        dependencies: [run-transformation-pipeline]
        arguments:
          parameters:
          - {name: gcs_root, value: '{{inputs.parameters.gcs_root}}'}
          - {name: hypertune_settings, value: '{{inputs.parameters.hypertune_settings}}'}
          - {name: num_epochs_hypertune, value: '{{inputs.parameters.num_epochs_hypertune}}'}
          - {name: num_units, value: '{{inputs.parameters.num_units}}'}
          - {name: project_id, value: '{{inputs.parameters.project_id}}'}
          - {name: region, value: '{{inputs.parameters.region}}'}
          - {name: run-transformation-pipeline-training_file_path, value: '{{tasks.run-transformation-pipeline.outputs.parameters.run-transformation-pipeline-training_file_path}}'}
          - {name: run-transformation-pipeline-transform_artefacts_dir, value: '{{tasks.run-transformation-pipeline.outputs.parameters.run-transformation-pipeline-transform_artefacts_dir}}'}
          - {name: run-transformation-pipeline-validation_file_path, value: '{{tasks.run-transformation-pipeline.outputs.parameters.run-transformation-pipeline-validation_file_path}}'}
      - name: submitting-a-cloud-ml-training-job-as-a-pipeline-step-2
        template: submitting-a-cloud-ml-training-job-as-a-pipeline-step-2
        dependencies: [retrieve-best-run, run-transformation-pipeline]
        arguments:
          parameters:
          - {name: gcs_root, value: '{{inputs.parameters.gcs_root}}'}
          - {name: num_epochs_retrain, value: '{{inputs.parameters.num_epochs_retrain}}'}
          - {name: num_units, value: '{{inputs.parameters.num_units}}'}
          - {name: project_id, value: '{{inputs.parameters.project_id}}'}
          - {name: region, value: '{{inputs.parameters.region}}'}
          - {name: retrieve-best-run-dropout_rate, value: '{{tasks.retrieve-best-run.outputs.parameters.retrieve-best-run-dropout_rate}}'}
          - {name: retrieve-best-run-learning_rate, value: '{{tasks.retrieve-best-run.outputs.parameters.retrieve-best-run-learning_rate}}'}
          - {name: run-transformation-pipeline-training_file_path, value: '{{tasks.run-transformation-pipeline.outputs.parameters.run-transformation-pipeline-training_file_path}}'}
          - {name: run-transformation-pipeline-transform_artefacts_dir, value: '{{tasks.run-transformation-pipeline.outputs.parameters.run-transformation-pipeline-transform_artefacts_dir}}'}
          - {name: run-transformation-pipeline-validation_file_path, value: '{{tasks.run-transformation-pipeline.outputs.parameters.run-transformation-pipeline-validation_file_path}}'}
  arguments:
    parameters:
    - {name: project_id}
    - {name: gcs_root}
    - {name: region}
    - {name: source_table_name}
    - {name: num_epochs_hypertune}
    - {name: num_epochs_retrain}
    - {name: num_units}
    - {name: evaluation_metric_name}
    - {name: evaluation_metric_threshold}
    - {name: model_id}
    - {name: version_id}
    - {name: replace_existing_version}
    - name: hypertune_settings
      value: |2

        {
            "hyperparameters": {
                "goal": "MINIMIZE",
                "maxTrials": 3,
                "maxParallelTrials": 3,
                "hyperparameterMetricTag": "val_loss",
                "enableTrialEarlyStopping": True,
                "params":[
                    {
                        "parameterName": "learning_rate",
                        "type": "DOUBLE",
                        "minValue": 0.00001,
                        "maxValue": 0.1,
                        "scaleType": "UNIT_LOG_SCALE"
                    },
                    {
                        "parameterName": "dropout_rate",
                        "type": "DOUBLE",
                        "minValue": 0.1,
                        "maxValue": 0.4,
                        "scaleType": "UNIT_LOG_SCALE"
                    }
                ]
            }
        }
  serviceAccountName: pipeline-runner
